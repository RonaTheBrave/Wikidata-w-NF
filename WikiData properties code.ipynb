{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498d570-ff8f-4408-a458-8165815a57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia setuptools pywikibot mwparserfromhell pandas numpy scipy nltk tqdm seaborn pyqt5 pyqtwebengine ruamel-yaml lxml datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8d29448-4a36-42bc-987e-bf8bd2b8e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "import pywikibot\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f68f6d59-0199-4db8-89d7-abf6fd9d01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"en\")  #Limit to English WP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4022b6d-a167-4250-8516-c1d5c4902314",
   "metadata": {},
   "source": [
    "## Part1. Corpus builder. Define functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e36210-e324-4b02-ade0-7da445c6831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_the_titles(key_words : str,number_of_results):\n",
    "    ''' \n",
    "    function to get the title of pages using wikipedia search with keywords\n",
    "    \n",
    "    :param key_words: words to effectuate the wiki research of pages\n",
    "    \n",
    "    :param number of results: number of pages the user want to scrap maximum 500\n",
    "    :type number of results: int\n",
    "    \n",
    "    :returns: title of the pages and the number of results\n",
    "    :rtype: list, int\n",
    "    '''\n",
    "    titles = []\n",
    "    n=str(number_of_results)\n",
    "    #the url understand + and not space\n",
    "    a = key_words.lower().replace(' ', '+')\n",
    "    scraped_url = 'https://en.wikipedia.org/w/index.php?title=Special:Search&limit='+n+'&offset=0&profile=default&search={}&ns0=1'.format(a)\n",
    "    html_text = requests.get(scraped_url).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    r = soup.find('div', {'class' : 'results-info'})\n",
    "    nbr_result = int(r.get('data-mw-num-results-total'))\n",
    "    for interest in soup.find_all('div', {'class' : 'mw-search-result-heading'}):\n",
    "        titles.append(interest.find('a')['title']) \n",
    "    return titles, nbr_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71719b0a-3a54-4a84-8fa3-4177294ee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_titles(page : str):\n",
    "    '''\n",
    "    Returns the section titles\n",
    "\n",
    "    :param page: Title of the page\n",
    "    \n",
    "    :returns: The list of the titles of the sections\n",
    "    :rtype: list\n",
    "    '''\n",
    "    section_tit = set()\n",
    "    section_title = []\n",
    "    scraped_url = wikipedia.page(page, auto_suggest = False).url\n",
    "    \n",
    "    html_text = requests.get(scraped_url).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    #find all the headings in the html text \n",
    "    r = soup.find_all('h2') + soup.find_all('h3') + soup.find_all('h4') \n",
    "    for i in r:\n",
    "        if i.find('span', {'class' : 'mw-headline'}) is not None:\n",
    "            section_tit.add(i.find('span', {'class' : 'mw-headline'})['id'])\n",
    "    for i in section_tit:\n",
    "        section_title.append(i.replace('_', ' '))        \n",
    "    return section_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ddd7ed-0332-4101-af2e-54b28e9640be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define a function to automatically set up a corpus of related articles\n",
    "def corpus_selection(word_keys : str , number_of_results : int, key_for_section : str):\n",
    "    '''\n",
    "    Select the corpus if the keywords are in the section or in the title\n",
    "\n",
    "    :param word_keys : words to launch the wikipedia search with\n",
    "    \n",
    "    :param number_of_results: number of results the user wants for their corpus maximum 500\n",
    "    \n",
    "    :param key_for_section: key words to be found in the titles or in the sections\n",
    "    \n",
    "    :returns: a list containing the title of all the pages of the corpus   \n",
    "    :rtype: list\n",
    "    '''\n",
    "    corpus = []\n",
    "    proper_list = []\n",
    "    left_list = []\n",
    "    title_list=[]\n",
    "    function = get_all_the_titles(word_keys,number_of_results)\n",
    "    nbr_result = function[1]\n",
    "    search_list = function[0] #Select and put in a list number_of_results wikipedia articles related to word_keys\n",
    "    \n",
    "    \n",
    "    for article in tqdm(search_list):\n",
    "        #create a new list avoiding disambiguation errors\n",
    "        try:\n",
    "            wikipedia.summary(article, auto_suggest = False)\n",
    "            proper_list.append(article)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            pass\n",
    "        #sometimes gets uncategorised errors in that case retry twice then pass\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            try:\n",
    "                wikipedia.summary(article, auto_suggest = False)\n",
    "                proper_list.append(article)\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                pass\n",
    "            \n",
    "    #add to the corpus the article which contains key words in their title\n",
    "    print(\"finding pages with key words in the title\")\n",
    "    for proper_article in tqdm(proper_list):\n",
    "        if key_for_section in proper_article.lower(): #Among these articles, select the ones whose title contains key_for_selection for the corpus\n",
    "            corpus.append(proper_article)\n",
    "        else:\n",
    "            left_list.append(proper_article) #Put the rest of the articles in a list\n",
    "    \n",
    "    #add to the corpus the articles which contains key words in their sections\n",
    "    print(\"finding pages with key words in the sections\")\n",
    "    for left_article in tqdm(left_list):\n",
    "        sec_tit = []\n",
    "        try :\n",
    "            sec_tit = get_section_titles(left_article)\n",
    "            for section in sec_tit:\n",
    "                if key_for_section.lower() in section.lower() and left_article not in corpus:\n",
    "                    corpus.append(left_article)\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            pass\n",
    "        except wikipedia.exceptions.PageError as e:\n",
    "            try:\n",
    "                sec_tit = get_section_titles(left_article)\n",
    "                for section in sec_tit:\n",
    "                    if key_for_section.lower() in section.lower() and left_article not in corpus:\n",
    "                        corpus.append(left_article)\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                pass\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d4f05-afc3-40d5-bf56-e640696f5623",
   "metadata": {},
   "source": [
    "## Part1. Corpus builder. Operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e585c23f-0960-4bb3-90bf-16843f0900db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [00:11<00:00, 42.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding pages with key words in the title\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 498/498 [00:00<00:00, 84336.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding pages with key words in the sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 497/497 [17:09<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 3.48 s, total: 1min 43s\n",
      "Wall time: 17min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Urban heat island']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Run the corpus creation: \n",
    "# in TOI insert the Term Of Interst, use \"key_for_section\" for a subspecification (or same as TOI)\n",
    "# diminished the number of results to 500 (instead of 5000) for faster checking\n",
    "TOI = \"urban heat island\" \n",
    "search_list = corpus_selection(word_keys = TOI, number_of_results = 500, key_for_section = \"heat island\")\n",
    "search_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb104c2b-a16b-4d57-9211-67e9d4871469",
   "metadata": {},
   "source": [
    "#### creation of a dataframe with informations on the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "38f74ec5-33d3-45fc-8e4b-15ed53039d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: <unknown>:22: SyntaxWarning: invalid escape sequence '\\w'\n",
      "\n",
      "WARNING: <unknown>:24: SyntaxWarning: invalid escape sequence '\\w'\n",
      "\n",
      "WARNING: <unknown>:26: SyntaxWarning: invalid escape sequence '\\w'\n",
      "\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:01<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 ms, sys: 108 ms, total: 150 ms\n",
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \n",
       "0  {{Short description|Situation where cities are...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def crea_dataframe(search_list : list,keyword : str, section_search = False)-> pd.DataFrame:\n",
    "    '''\n",
    "    :param search_list: liste containing the name of all the articles\n",
    "    :param keyword: string containing the keyword that we want to scrap\n",
    "    \n",
    "    :param section_search: if True extract the wikicode only of the section containing the keyword in it's name\n",
    "    :type section_search: bool \n",
    "     \n",
    "    :returns: data frame containg the name, the url and the wikicode of the entire page if the keyword is in the title.\n",
    "    If it's not it returns the wiki code of the section containing the keyword only. Or return the wikicode for each entire page if section_search=False\n",
    "    :rtype: Dataframe \n",
    "    '''\n",
    "    tableau =[] #creation of a list that will contain a dictionnary for each page with the information\n",
    "\n",
    "    \n",
    "    if section_search:    \n",
    "        '''\n",
    "        It detects if the keyword is included in a section, a subsection or a subsubsection\n",
    "        '''\n",
    "        #use re to find the section in the page text\n",
    "        #section\n",
    "        recode= r'(?:==(?:\\w|\\ )*?(?:'+keyword[0].lower()+'|'+keyword[0].upper()+')'+keyword[1:]+'(?:\\w|\\ )*?==\\n)((?:.|\\n)*?)(?:==(?:\\w|\\ )*==\\n)'\n",
    "        #subsection\n",
    "        recode2=r'(?:===(?:\\w|\\ )*?(?:'+keyword[0].lower()+'|'+keyword[0].upper()+')'+keyword[1:]+'(?:\\w|\\ )*?===\\n)((?:.|\\n)*?)(?:(?:===|==)(?:\\w|\\ )*(?:===|==)\\n)'\n",
    "        #subsubsection\n",
    "        recode3=r'(?:====(?:\\w|\\ )*?(?:'+keyword[0].lower()+'|'+keyword[0].upper()+')'+keyword[1:]+'(?:\\w|\\ )*?====\\n)((?:.|\\n)*?)(?:(?:===|==|====)(?:\\w|\\ )*(?:===|==|====)\\n)'\n",
    "    \n",
    "    #browse all the titles  of the search list\n",
    "    for i in tqdm(range(len(search_list))):\n",
    "             \n",
    "        #find the wikipedia page\n",
    "        page = wikipedia.page(search_list[i], auto_suggest = False)\n",
    "        page_title =page.title #give the clean name of the page\n",
    "        page_url= page.url #give the url of the page\n",
    "        site = pywikibot.Site(\"en\", \"wikipedia\")\n",
    "        page = pywikibot.Page(site, page_title)    \n",
    "                \n",
    "        if section_search:\n",
    "            if keyword.lower() in page_title.lower(): \n",
    "                page_text=page.text\n",
    "            else:\n",
    "                page_text = str(re.findall(recode , page.text))\n",
    "            if page_text==\"[]\":\n",
    "                page_text = str(re.findall(recode2, page.text))\n",
    "            if page_text== \"[]\":\n",
    "                page_text = str(re.findall(recode3, page.text))\n",
    "        else:\n",
    "            page_text=page.text\n",
    "            \n",
    "        infopage ={'Name only' : page_title, 'page url' :page_url,'text':page_text}\n",
    "        tableau.append(infopage)\n",
    "    df=pd.DataFrame.from_dict(tableau)\n",
    "    return df\n",
    "\n",
    "\n",
    "df=crea_dataframe(search_list,TOI)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095372a-e4a1-441d-8a78-f0b0fcdf6919",
   "metadata": {},
   "source": [
    "## Adding WikiData page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8291ec7-2b07-493c-87e2-27a99d750516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url  \n",
       "0  https://www.wikidata.org/wiki/Q215712  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_wikidata_url(wikipedia_url):\n",
    "    # Extract the article title from the URL\n",
    "    title = wikipedia_url.split('/wiki/')[-1]\n",
    "    # Query the Wikidata API\n",
    "    response = requests.get(f'https://www.wikidata.org/w/api.php?action=wbgetentities&sites=enwiki&titles={title}&format=json')\n",
    "    data = response.json()\n",
    "    entities = data.get('entities')\n",
    "    if entities:\n",
    "        entity_id = list(entities.keys())[0]\n",
    "        if entity_id != '-1':\n",
    "            return f'https://www.wikidata.org/wiki/{entity_id}'\n",
    "    return ''\n",
    "\n",
    "df_WD = df.copy()\n",
    "# Apply the function to the DataFrame\n",
    "df_WD['wikidata_url'] = df_WD['page url'].apply(get_wikidata_url)\n",
    "\n",
    "df_WD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d1695-1e09-4d33-812f-dfd651bb1519",
   "metadata": {},
   "source": [
    "## Adding the Creation Dates of Wikipedia and Wikidata pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1d91960-1e43-4104-a57d-e99fff641bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>Wikipedia Creation Date</th>\n",
       "      <th>Wikidata Creation Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>2001-12-06 02:06:31</td>\n",
       "      <td>2012-11-30 10:10:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url Wikipedia Creation Date  \\\n",
       "0  https://www.wikidata.org/wiki/Q215712     2001-12-06 02:06:31   \n",
       "\n",
       "  Wikidata Creation Date  \n",
       "0    2012-11-30 10:10:43  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get Wikipedia article creation date\n",
    "def get_wikipedia_creation_date(page_url):\n",
    "    if pd.isna(page_url):\n",
    "        return None\n",
    "    page_title = page_url.split('/')[-1]\n",
    "    endpoint = f\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvlimit=1&rvdir=newer&titles={page_title}&format=json\"\n",
    "\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "    page_id = next(iter(data['query']['pages']))\n",
    "\n",
    "    if 'revisions' in data['query']['pages'][page_id]:\n",
    "        creation_date = data['query']['pages'][page_id]['revisions'][0]['timestamp']\n",
    "        creation_date = datetime.strptime(creation_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        return creation_date\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to get Wikidata item creation date\n",
    "def get_wikidata_creation_date(wikidata_url):\n",
    "    if pd.isna(wikidata_url):\n",
    "        return None\n",
    "    entity_id = wikidata_url.split('/')[-1]\n",
    "    endpoint = f\"https://www.wikidata.org/w/api.php?action=query&prop=revisions&rvlimit=1&rvdir=newer&titles=Item:{entity_id}&format=json\"\n",
    "\n",
    "    response = requests.get(endpoint)\n",
    "    data = response.json()\n",
    "    page_id = next(iter(data['query']['pages']))\n",
    "\n",
    "    if 'revisions' in data['query']['pages'][page_id]:\n",
    "        creation_date = data['query']['pages'][page_id]['revisions'][0]['timestamp']\n",
    "        creation_date = datetime.strptime(creation_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        return creation_date\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_DOB = df_WD.copy()\n",
    "# Add new columns for creation dates\n",
    "df_DOB['Wikipedia Creation Date'] = df_DOB['page url'].apply(get_wikipedia_creation_date)\n",
    "df_DOB['Wikidata Creation Date'] = df_DOB['wikidata_url'].apply(get_wikidata_creation_date)\n",
    "\n",
    "df_DOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910e4e3-5d4b-47c8-90db-66ea62dd6c8a",
   "metadata": {},
   "source": [
    "## Adding the first level of WD properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db3eb79d-08c4-42fe-af6f-f1cc2d4c3524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>instance_of</th>\n",
       "      <th>part_of</th>\n",
       "      <th>subclass_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>Q215712</td>\n",
       "      <td>Q483247</td>\n",
       "      <td></td>\n",
       "      <td>Q702492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url wikidata_id instance_of part_of  \\\n",
       "0  https://www.wikidata.org/wiki/Q215712     Q215712     Q483247           \n",
       "\n",
       "  subclass_of  \n",
       "0     Q702492  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the correct column name for Wikidata URLs\n",
    "wikidata_url_column = 'wikidata_url'  # Update this if the column name is different\n",
    "\n",
    "# Function to fetch Wikidata properties\n",
    "def fetch_wikidata_properties(wikidata_url):\n",
    "    if pd.isna(wikidata_url):\n",
    "        return {}, {}, {}\n",
    "\n",
    "    entity_id = wikidata_url.split('/wiki/')[-1]\n",
    "    url = f'https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    claims = data['entities'][entity_id]['claims']\n",
    "\n",
    "    instance_of = claims.get('P31', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id', '')\n",
    "    part_of = claims.get('P361', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id', '')\n",
    "    subclass_of = claims.get('P279', [{}])[0].get('mainsnak', {}).get('datavalue', {}).get('value', {}).get('id', '')\n",
    "\n",
    "    return instance_of, part_of, subclass_of\n",
    "\n",
    "df_first_level = df_WD.copy()\n",
    "\n",
    "# Fetch properties for each Wikidata item and add them to the DataFrame\n",
    "df_first_level[['instance_of', 'part_of', 'subclass_of']] = df_first_level[wikidata_url_column].apply(\n",
    "    lambda url: pd.Series(fetch_wikidata_properties(url)))\n",
    "\n",
    "df_first_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0e8a4-eb16-45ac-8987-b8e3ed732100",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieving the whole hierarchy of \"subclass of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb591d30-4ba6-442b-b524-e98483eea3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 1/1 [00:23<00:00, 23.79s/item]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>Wikipedia Creation Date</th>\n",
       "      <th>Wikidata Creation Date</th>\n",
       "      <th>subclass_of_hierarchy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>2001-12-06 02:06:31</td>\n",
       "      <td>2012-11-30 10:10:43</td>\n",
       "      <td>[Q702492, Q82794, Q486972, Q618123, Q123964505...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url Wikipedia Creation Date  \\\n",
       "0  https://www.wikidata.org/wiki/Q215712     2001-12-06 02:06:31   \n",
       "\n",
       "  Wikidata Creation Date                              subclass_of_hierarchy  \n",
       "0    2012-11-30 10:10:43  [Q702492, Q82794, Q486972, Q618123, Q123964505...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Wikidata IDs from the 'wikidata_url' column\n",
    "df_WD['wikidata_url'] = df_WD['wikidata_url'].astype(str)\n",
    "df_WD['wikidata_id'] = df_WD['wikidata_url'].apply(lambda x: re.search(r'Q\\d+', x).group() if re.search(r'Q\\d+', x) else None)\n",
    "\n",
    "\n",
    "# Function to get the \"subclass of\" hierarchy for a given Wikidata item ID (iterative approach)\n",
    "def get_subclass_of_hierarchy(item_id):\n",
    "    hierarchy = []\n",
    "    stack = [item_id]  # Using a stack for iterative depth-first search\n",
    "    while stack:\n",
    "        current_id = stack.pop()\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={current_id}&format=json&props=claims\"\n",
    "        response = requests.get(url).json()\n",
    "        if 'entities' in response and current_id in response['entities']:\n",
    "            claims = response['entities'][current_id].get('claims', {})\n",
    "            if 'P279' in claims:  # P279 is \"subclass of\"\n",
    "                subclass_of_ids = [claim['mainsnak'].get('datavalue', {}).get('value', {}).get('id') for claim in claims['P279'] if claim['mainsnak'].get('datavalue')]\n",
    "                for subclass_of_id in subclass_of_ids:\n",
    "                    if subclass_of_id not in hierarchy:\n",
    "                        hierarchy.append(subclass_of_id)\n",
    "                        stack.append(subclass_of_id)\n",
    "    return hierarchy\n",
    "\n",
    "results = []\n",
    "for i, wikidata_id in enumerate(tqdm(df_WD['wikidata_id'], desc='Processing', unit='item')):\n",
    "    if wikidata_id:\n",
    "        hierarchy = get_subclass_of_hierarchy(wikidata_id)\n",
    "    else:\n",
    "        hierarchy = []\n",
    "    results.append(hierarchy)\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "df_subclass_of = df_DOB.copy()\n",
    "df_subclass_of['subclass_of_hierarchy'] = results\n",
    "\n",
    "df_subclass_of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff4013-ec1c-48c4-8f60-a791eb159ea6",
   "metadata": {},
   "source": [
    "## Retrieving the whole hierarchy of \"part of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cb80966d-d81a-4177-95c0-5ec52b8adef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 1/1 [00:00<00:00,  1.97item/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>Wikipedia Creation Date</th>\n",
       "      <th>Wikidata Creation Date</th>\n",
       "      <th>part_of_hierarchy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>2001-12-06 02:06:31</td>\n",
       "      <td>2012-11-30 10:10:43</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url Wikipedia Creation Date  \\\n",
       "0  https://www.wikidata.org/wiki/Q215712     2001-12-06 02:06:31   \n",
       "\n",
       "  Wikidata Creation Date part_of_hierarchy  \n",
       "0    2012-11-30 10:10:43                []  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get the \"part of\" hierarchy for a given Wikidata item ID (iterative approach)\n",
    "def get_part_of_hierarchy(item_id):\n",
    "    hierarchy = []\n",
    "    stack = [item_id]  # Using a stack for iterative depth-first search\n",
    "    while stack:\n",
    "        current_id = stack.pop()\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={current_id}&format=json&props=claims\"\n",
    "        response = requests.get(url).json()\n",
    "        if 'entities' in response and current_id in response['entities']:\n",
    "            claims = response['entities'][current_id].get('claims', {})\n",
    "            if 'P361' in claims:  # P361 is \"part of\"\n",
    "                part_of_ids = [claim['mainsnak'].get('datavalue', {}).get('value', {}).get('id') for claim in claims['P361'] if claim['mainsnak'].get('datavalue')]\n",
    "                for part_of_id in part_of_ids:\n",
    "                    if part_of_id not in hierarchy:\n",
    "                        hierarchy.append(part_of_id)\n",
    "                        stack.append(part_of_id)\n",
    "    return hierarchy\n",
    "\n",
    "results_part_of = []\n",
    "\n",
    "for i, wikidata_id in enumerate(tqdm(df_WD['wikidata_id'], desc='Processing', unit='item')):\n",
    "    if wikidata_id:\n",
    "        hierarchy = get_part_of_hierarchy(wikidata_id)\n",
    "    else:\n",
    "        hierarchy = []\n",
    "    results_part_of.append(hierarchy)\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "df_part_of = df_DOB.copy()\n",
    "df_part_of['part_of_hierarchy'] = results_part_of\n",
    "\n",
    "df_part_of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6e7e1-93fc-41ee-9b15-c28165cba066",
   "metadata": {},
   "source": [
    "## Retrieving the whole hierarchy of \"instance of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10eb6396-456f-4447-8dae-e7321a12a300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 1/1 [00:00<00:00,  1.05item/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>instance_of_hierarchy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>Q215712</td>\n",
       "      <td>[Q483247]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url wikidata_id instance_of_hierarchy  \n",
       "0  https://www.wikidata.org/wiki/Q215712     Q215712             [Q483247]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get the \"instance of\" hierarchy for a given Wikidata item ID (iterative approach)\n",
    "def get_instance_of_hierarchy(item_id):\n",
    "    hierarchy = []\n",
    "    stack = [item_id]  # Using a stack for iterative depth-first search\n",
    "    while stack:\n",
    "        current_id = stack.pop()\n",
    "        url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={current_id}&format=json&props=claims\"\n",
    "        response = requests.get(url).json()\n",
    "        if 'entities' in response and current_id in response['entities']:\n",
    "            claims = response['entities'][current_id].get('claims', {})\n",
    "            if 'P31' in claims:  # P31 is \"instance of\"\n",
    "                instance_of_ids = [claim['mainsnak'].get('datavalue', {}).get('value', {}).get('id') for claim in claims['P31'] if claim['mainsnak'].get('datavalue')]\n",
    "                for instance_of_id in instance_of_ids:\n",
    "                    if instance_of_id not in hierarchy:\n",
    "                        hierarchy.append(instance_of_id)\n",
    "                        stack.append(instance_of_id)\n",
    "    return hierarchy\n",
    "\n",
    "results_instance_of = []\n",
    "\n",
    "for i, wikidata_id in enumerate(tqdm(df_WD['wikidata_id'], desc='Processing', unit='item')):\n",
    "    if wikidata_id:\n",
    "        hierarchy = get_instance_of_hierarchy(wikidata_id)\n",
    "    else:\n",
    "        hierarchy = []\n",
    "    results_instance_of.append(hierarchy)\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "df_instance_of = df_WD.copy()\n",
    "df_instance_of['instance_of_hierarchy'] = results_instance_of\n",
    "df_instance_of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa787648-b6a1-4b96-95aa-dda72d8eea9d",
   "metadata": {},
   "source": [
    "## Retrieving labels of the Wikidata articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5b4cb099-220f-4c1d-92b8-986b3e65e072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name only</th>\n",
       "      <th>page url</th>\n",
       "      <th>text</th>\n",
       "      <th>wikidata_url</th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>instance_of</th>\n",
       "      <th>part_of</th>\n",
       "      <th>subclass_of</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban heat island</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Urban_heat_island</td>\n",
       "      <td>{{Short description|Situation where cities are...</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q215712</td>\n",
       "      <td>Q215712</td>\n",
       "      <td>phenomenon</td>\n",
       "      <td>None</td>\n",
       "      <td>urban area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name only                                         page url  \\\n",
       "0  Urban heat island  https://en.wikipedia.org/wiki/Urban_heat_island   \n",
       "\n",
       "                                                text  \\\n",
       "0  {{Short description|Situation where cities are...   \n",
       "\n",
       "                            wikidata_url wikidata_id instance_of part_of  \\\n",
       "0  https://www.wikidata.org/wiki/Q215712     Q215712  phenomenon    None   \n",
       "\n",
       "  subclass_of  \n",
       "0  urban area  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get a label for a Wikidata ID\n",
    "def get_wikidata_label(wikidata_id):\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        try:\n",
    "            return data['entities'][wikidata_id]['labels']['en']['value']\n",
    "        except KeyError:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define column names to process\n",
    "columns_to_process = df_first_level[['instance_of', 'part_of', 'subclass_of']] #change to the necessary columns\n",
    "\n",
    "# Extract unique Wikidata IDs from the selected columns\n",
    "unique_wikidata_ids = pd.unique(columns_to_process.values.ravel('K')).tolist()\n",
    "unique_wikidata_ids = [x for x in unique_wikidata_ids if pd.notna(x)]\n",
    "\n",
    "# Get labels for all unique Wikidata IDs\n",
    "wikidata_labels = {wid: get_wikidata_label(wid) for wid in unique_wikidata_ids}\n",
    "\n",
    "# Replace Wikidata IDs with their labels in the selected columns\n",
    "df_labeled = df_first_level.copy()\n",
    "\n",
    "for column in columns_to_process:\n",
    "    df_labeled[column] = df_labeled[column].apply(lambda wid: wikidata_labels.get(wid, wid))\n",
    "\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d910d08-57c5-4483-9d95-6cf324d41715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
